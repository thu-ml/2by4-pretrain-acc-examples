Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 64
Overriding: n_head = 12
Overriding: n_embd = 768
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 45.71M
iter 0: time 7718.68ms
iter 1: time 8317.08ms
iter 2: time 8348.58ms
iter 3: time 8376.75ms
iter 4: time 8397.75ms
iter 5: time 8431.11ms
iter 6: time 8439.79ms
iter 7: time 8447.90ms
iter 8: time 8457.21ms
iter 9: time 8464.70ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 12
Overriding: n_embd = 768
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 45.71M
iter 0: time 4050.14ms
iter 1: time 4219.34ms
iter 2: time 4222.12ms
iter 3: time 4223.10ms
iter 4: time 4222.91ms
iter 5: time 4224.84ms
iter 6: time 4225.17ms
iter 7: time 4227.24ms
iter 8: time 4230.91ms
iter 9: time 4230.69ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 12
Overriding: n_embd = 768
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 45.71M
iter 0: time 2198.00ms
iter 1: time 2134.03ms
iter 2: time 2135.42ms
iter 3: time 2135.98ms
iter 4: time 2136.23ms
iter 5: time 2135.95ms
iter 6: time 2138.34ms
iter 7: time 2138.09ms
iter 8: time 2137.83ms
iter 9: time 2139.42ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 12
Overriding: n_embd = 768
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 45.71M
iter 0: time 1271.37ms
iter 1: time 1095.94ms
iter 2: time 1097.15ms
iter 3: time 1097.02ms
iter 4: time 1098.31ms
iter 5: time 1098.81ms
iter 6: time 1098.82ms
iter 7: time 1097.51ms
iter 8: time 1096.89ms
iter 9: time 1097.94ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 12
Overriding: n_embd = 768
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 45.71M
iter 0: time 803.33ms
iter 1: time 587.81ms
iter 2: time 592.69ms
iter 3: time 589.24ms
iter 4: time 590.95ms
iter 5: time 590.09ms
iter 6: time 590.17ms
iter 7: time 591.56ms
iter 8: time 589.02ms
iter 9: time 591.57ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 64
Overriding: n_head = 16
Overriding: n_embd = 1024
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 64.10M
iter 0: time 12026.04ms
iter 1: time 13612.16ms
iter 2: time 13631.17ms
iter 3: time 13641.87ms
iter 4: time 13649.14ms
iter 5: time 13654.63ms
iter 6: time 13656.61ms
iter 7: time 13658.66ms
iter 8: time 13658.78ms
iter 9: time 13657.86ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 16
Overriding: n_embd = 1024
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 64.10M
iter 0: time 6201.65ms
iter 1: time 6815.36ms
iter 2: time 6824.54ms
iter 3: time 6825.11ms
iter 4: time 6829.07ms
iter 5: time 6824.19ms
iter 6: time 6826.38ms
iter 7: time 6831.03ms
iter 8: time 6836.26ms
iter 9: time 6831.61ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 16
Overriding: n_embd = 1024
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 64.10M
iter 0: time 3253.00ms
iter 1: time 3380.33ms
iter 2: time 3382.42ms
iter 3: time 3381.45ms
iter 4: time 3383.65ms
iter 5: time 3383.77ms
iter 6: time 3386.91ms
iter 7: time 3387.36ms
iter 8: time 3386.04ms
iter 9: time 3385.61ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 16
Overriding: n_embd = 1024
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 64.10M
iter 0: time 1797.72ms
iter 1: time 1733.80ms
iter 2: time 1737.82ms
iter 3: time 1736.49ms
iter 4: time 1737.00ms
iter 5: time 1738.45ms
iter 6: time 1738.80ms
iter 7: time 1738.03ms
iter 8: time 1737.77ms
iter 9: time 1738.87ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 16
Overriding: n_embd = 1024
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 64.10M
iter 0: time 1092.15ms
iter 1: time 912.55ms
iter 2: time 913.14ms
iter 3: time 914.12ms
iter 4: time 913.59ms
iter 5: time 914.89ms
iter 6: time 915.02ms
iter 7: time 914.55ms
iter 8: time 913.69ms
iter 9: time 915.04ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 64
Overriding: n_head = 32
Overriding: n_embd = 2048
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 153.36M
iter 0: time 36128.62ms
iter 1: time 45085.44ms
iter 2: time 45154.77ms
iter 3: time 45172.67ms
iter 4: time 45179.83ms
iter 5: time 45191.38ms
iter 6: time 45188.94ms
iter 7: time 45181.32ms
iter 8: time 45184.74ms
iter 9: time 45182.94ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 32
Overriding: n_embd = 2048
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 153.36M
iter 0: time 18322.60ms
iter 1: time 22524.47ms
iter 2: time 22554.85ms
iter 3: time 22569.46ms
iter 4: time 22575.43ms
iter 5: time 22586.03ms
iter 6: time 22582.81ms
iter 7: time 22580.30ms
iter 8: time 22578.17ms
iter 9: time 22581.82ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 32
Overriding: n_embd = 2048
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 153.36M
iter 0: time 9238.37ms
iter 1: time 10998.56ms
iter 2: time 11004.40ms
iter 3: time 11007.48ms
iter 4: time 11011.42ms
iter 5: time 11012.22ms
iter 6: time 11018.79ms
iter 7: time 11016.70ms
iter 8: time 11017.89ms
iter 9: time 11021.77ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 32
Overriding: n_embd = 2048
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 153.36M
iter 0: time 4918.87ms
iter 1: time 5603.24ms
iter 2: time 5607.26ms
iter 3: time 5613.16ms
iter 4: time 5614.81ms
iter 5: time 5618.79ms
iter 6: time 5622.50ms
iter 7: time 5620.08ms
iter 8: time 5624.76ms
iter 9: time 5623.67ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 32
Overriding: n_embd = 2048
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 153.36M
iter 0: time 2673.61ms
iter 1: time 2886.77ms
iter 2: time 2896.78ms
iter 3: time 2899.21ms
iter 4: time 2899.79ms
iter 5: time 2900.28ms
iter 6: time 2900.78ms
iter 7: time 2900.45ms
iter 8: time 2902.27ms
iter 9: time 2902.94ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 64
Overriding: n_head = 64
Overriding: n_embd = 2560
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 207.43M
iter 0: time 55502.47ms
iter 1: time 70243.13ms
iter 2: time 70316.54ms
iter 3: time 70303.76ms
iter 4: time 70316.63ms
iter 5: time 70331.94ms
iter 6: time 70371.53ms
iter 7: time 70357.70ms
iter 8: time 70337.07ms
iter 9: time 70341.09ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 32
Overriding: n_embd = 2560
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 207.43M
iter 0: time 26671.12ms
iter 1: time 33961.54ms
iter 2: time 34002.78ms
iter 3: time 34021.25ms
iter 4: time 34033.97ms
iter 5: time 34036.70ms
iter 6: time 34038.83ms
iter 7: time 34041.46ms
iter 8: time 34042.34ms
iter 9: time 34041.58ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 32
Overriding: n_embd = 2560
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 207.43M
iter 0: time 13497.48ms
iter 1: time 16715.42ms
iter 2: time 16734.53ms
iter 3: time 16752.59ms
iter 4: time 16758.41ms
iter 5: time 16761.09ms
iter 6: time 16762.69ms
iter 7: time 16761.43ms
iter 8: time 16765.14ms
iter 9: time 16767.13ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 32
Overriding: n_embd = 2560
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 207.43M
iter 0: time 6989.06ms
iter 1: time 8513.87ms
iter 2: time 8531.39ms
iter 3: time 8543.54ms
iter 4: time 8551.89ms
iter 5: time 8552.07ms
iter 6: time 8554.59ms
iter 7: time 8557.16ms
iter 8: time 8555.80ms
iter 9: time 8557.45ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 32
Overriding: n_embd = 2560
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 207.43M
iter 0: time 3776.95ms
iter 1: time 4364.54ms
iter 2: time 4373.83ms
iter 3: time 4377.25ms
iter 4: time 4376.02ms
iter 5: time 4379.41ms
iter 6: time 4381.37ms
iter 7: time 4383.98ms
iter 8: time 4385.37ms
iter 9: time 4386.53ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 32
Overriding: n_embd = 4096
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 407.38M
iter 0: time 58791.76ms
iter 1: time 77884.81ms
iter 2: time 77941.23ms
iter 3: time 77962.11ms
iter 4: time 77973.82ms
iter 5: time 77951.41ms
iter 6: time 77978.61ms
iter 7: time 77965.83ms
iter 8: time 77983.23ms
iter 9: time 77969.23ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 32
Overriding: n_embd = 4096
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 407.38M
iter 0: time 29175.02ms
iter 1: time 38489.90ms
iter 2: time 38636.81ms
iter 3: time 38645.95ms
iter 4: time 38655.06ms
iter 5: time 38646.07ms
iter 6: time 38659.36ms
iter 7: time 38670.97ms
iter 8: time 38669.13ms
iter 9: time 38666.20ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 32
Overriding: n_embd = 4096
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 407.38M
iter 0: time 14751.13ms
iter 1: time 19477.96ms
iter 2: time 19527.81ms
iter 3: time 19596.45ms
iter 4: time 19613.48ms
iter 5: time 19614.93ms
iter 6: time 19621.29ms
iter 7: time 19629.77ms
iter 8: time 19635.11ms
iter 9: time 19638.27ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 32
Overriding: n_embd = 4096
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 407.38M
iter 0: time 7893.05ms
iter 1: time 10041.95ms
iter 2: time 10073.37ms
iter 3: time 10116.22ms
iter 4: time 10124.10ms
iter 5: time 10131.17ms
iter 6: time 10133.21ms
iter 7: time 10138.11ms
iter 8: time 10138.98ms
iter 9: time 10141.45ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 32
Overriding: n_head = 40
Overriding: n_embd = 5120
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 572.14M
Traceback (most recent call last):
  File "/root/accelerate/dense-block/train.py", line 128, in <module>
    scaler.scale(out).backward(Y)
  File "/root/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.50 GiB. GPU 0 has a total capacity of 23.69 GiB of which 855.69 MiB is free. Process 980713 has 22.85 GiB memory in use. Of the allocated memory 22.28 GiB is allocated by PyTorch, and 266.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 40
Overriding: n_embd = 5120
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 572.14M
iter 0: time 43929.95ms
iter 1: time 59170.48ms
iter 2: time 59281.79ms
iter 3: time 59304.57ms
iter 4: time 59322.69ms
iter 5: time 59304.84ms
iter 6: time 59320.51ms
iter 7: time 59313.35ms
iter 8: time 59306.89ms
iter 9: time 59520.55ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 40
Overriding: n_embd = 5120
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 572.14M
iter 0: time 22269.28ms
iter 1: time 30028.68ms
iter 2: time 30149.12ms
iter 3: time 30263.42ms
iter 4: time 30263.08ms
iter 5: time 30253.76ms
iter 6: time 30244.59ms
iter 7: time 30240.18ms
iter 8: time 30238.15ms
iter 9: time 30264.50ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 40
Overriding: n_embd = 5120
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 572.14M
iter 0: time 11593.59ms
iter 1: time 15333.14ms
iter 2: time 15373.08ms
iter 3: time 15397.33ms
iter 4: time 15412.48ms
iter 5: time 15445.23ms
iter 6: time 15483.34ms
iter 7: time 15488.34ms
iter 8: time 15490.47ms
iter 9: time 15489.46ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 16
Overriding: n_head = 56
Overriding: n_embd = 7168
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 977.16M
iter 0: time 81163.84ms
iter 1: time 112654.03ms
iter 2: time 112753.34ms
iter 3: time 112808.72ms
iter 4: time 112831.44ms
iter 5: time 112882.53ms
iter 6: time 112842.72ms
iter 7: time 112773.52ms
iter 8: time 112836.48ms
iter 9: time 112789.98ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 8
Overriding: n_head = 56
Overriding: n_embd = 7168
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 977.16M
iter 0: time 41109.51ms
iter 1: time 57039.24ms
iter 2: time 57185.57ms
iter 3: time 57194.52ms
iter 4: time 57205.22ms
iter 5: time 57199.57ms
iter 6: time 57187.58ms
iter 7: time 57198.34ms
iter 8: time 57214.63ms
iter 9: time 57195.08ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 56
Overriding: n_embd = 7168
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 977.16M
iter 0: time 21301.42ms
iter 1: time 28887.01ms
iter 2: time 29060.17ms
iter 3: time 29090.11ms
iter 4: time 29100.28ms
iter 5: time 29107.77ms
iter 6: time 29118.62ms
iter 7: time 29118.57ms
iter 8: time 29119.74ms
iter 9: time 29115.82ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 4
Overriding: n_head = 72
Overriding: n_embd = 9216
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 1482.85M
iter 0: time 33786.66ms
iter 1: time 47212.45ms
iter 2: time 47446.28ms
iter 3: time 47488.87ms
iter 4: time 47511.02ms
iter 5: time 47505.17ms
iter 6: time 47515.21ms
iter 7: time 47532.36ms
iter 8: time 47528.18ms
iter 9: time 47540.28ms
Overriding config with config/train_gpt2.py:
Overriding: compile = False
Overriding: wandb_log = False
Overriding: dataset = shakespeare_char
Overriding: batch_size = 1
Overriding: n_head = 96
Overriding: n_embd = 12288
Overriding: n_layer = 1
Overriding: block_size = 2048
number of parameters: 2430.11M
Traceback (most recent call last):
  File "/root/accelerate/dense-block/train.py", line 128, in <module>
    scaler.scale(out).backward(Y)
  File "/root/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/root/miniconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.25 GiB. GPU 0 has a total capacity of 23.69 GiB of which 383.69 MiB is free. Process 182336 has 23.31 GiB memory in use. Of the allocated memory 20.60 GiB is allocated by PyTorch, and 2.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
